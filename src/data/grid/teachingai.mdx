---
title: 'Teaching AI'
author: 'Demian'
publishDate: 2025-09-08T00:00:00Z
excerpt: Developing a platform to measure AI effectiveness in teaching
---

import { Image } from 'astro:assets';
import teachingAiVideo from '~/assets/images/grid_portfolio/teaching_ai.mp4';
import closedLoopImage from '~/assets/images/TeachingAI/close_loop.png';
import apiDiagramImage from '~/assets/images/TeachingAI/api_diagram.png';

# Teaching AI

## Overview

During my UIC research internship, I helped develop a platform whose goal was to obtain data from students to measure whether
AI was effective at teaching.

<div class="flex justify-center my-8">
  <div class="max-w-4xl w-3/4">
    <video loop muted playsinline class="w-full rounded-lg shadow-lg">
      <source src={teachingAiVideo} type="video/mp4" />
      Your browser does not support the video tag.
    </video>
  </div>
</div>

## Project Development

The goal was very ambitious, and the project existed only in concept. But they had to start somewhere. My main focus was creating
the API and the User Interface. We wanted a fast prototype, so I used FastAPI and Streamlit. In retrospect, these weren't
the most robust tools for this project.

The experience gave me much more respect for web development. Coming from a background in embedded systems, electronics, and
robotics, I wasn't fond of web development initially. But this task was quite a challenge.

## Technical Implementation

I had to connect the 2 GPUs we had to process the different models we were running: XpressiveAI (changing facial expressions),
EMO (emotion detection), gaze detection (heuristic method), and text-to-speech. The only computationally heavy processing was from XpressiveAI.
Running that model on the computer made the response incredibly laggy and not user-friendly.

## Results

It was a great experience getting a sense of what it's like to be the architect of a web platform. The end product was a very simple
prototype where you could ask a simple question and the AI teacher would respond according to the detected emotion and gaze.

The images below are my attempt at explaining how the API worked. I feel the closed-loop diagram explains the data flow pretty well.

<div class="flex justify-center my-8">
  <div class="max-w-4xl w-full">
    <Image src={closedLoopImage} alt="Teaching AI Closed Loop Diagram" class="rounded-lg shadow-lg w-full h-auto" />
    <p class="text-center text-sm text-gray-600 dark:text-gray-400">Close loop system</p>
  </div>
</div>

<div class="flex justify-center my-8">
  <div class="max-w-4xl w-full">
    <Image src={apiDiagramImage} alt="Teaching AI API Diagram" class="rounded-lg shadow-lg w-full h-auto" />
    <p class="text-center text-sm text-gray-600 dark:text-gray-400">API Diagram</p>
  </div>
</div>
